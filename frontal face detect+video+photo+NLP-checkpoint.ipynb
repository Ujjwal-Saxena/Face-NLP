{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath='D:/ujjwal.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascPath='S:/machine learning/build-face-dataset/haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read image\n",
    "image=cv2.imread(imagePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a hoar face\n",
    "face=cv2.CascadeClassifier(cascPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfaces= face.detectMultiScale(image, minSize=(35,35), scaleFactor=1.1, minNeighbors=5, flags=cv2.CASCADE_SCALE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y,w,h) in myfaces:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:376: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8f1a207820b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Faces Found\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:376: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow(\"Faces Found\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# capturing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "im=cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state,frame=im.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('my_image',frame) #frame is used to give colored video and gray is used to give black and white video\n",
    "cv2.waitKey(0)\n",
    "im.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video capturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "video=cv2.VideoCapture(0)\n",
    "q=1\n",
    "while True:\n",
    "    q=q+1\n",
    "    check, frame=video.read()\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('capturing', frame) #frame gives ccolored video and gray gives black and white video\n",
    "    key=cv2.waitKey(1)\n",
    "    if key==ord('q'):\n",
    "        break\n",
    "        \n",
    "print(q)\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlaod NLTK- natural language tool kit\n",
    "#textblog\n",
    "#tweepy-developers.tweeter.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import tweepy\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6e230a00a763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\udit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\udit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\udit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1934\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1936\u001b[0m     \u001b[1;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\udit\\appdata\\local\\programs\\python\\python37-32\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems=nltk.corpus.gutenberg.words('blake-poems.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get each word from poem we use FOR LOOP\n",
    "for words in poems:\n",
    "    print(words,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now each word ,space,or any character or symbol is referred as token\n",
    "AI=\"Hi, There! We are in session.This session is based on NLP. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, There! We are in session.This session is based on NLP. '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_token=word_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'There', '!', 'We', 'are', 'in', 'session.This', 'session', 'is', 'based', 'on', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "print(AI_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_token[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, There!', 'We are in session.This session is based on NLP.']\n"
     ]
    }
   ],
   "source": [
    " # to define sentence as a word we use:- sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "AI_token=sent_tokenize(AI)\n",
    "print(AI_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Hi, There!': 1, 'We are in session.This session is based on NLP.': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_token:\n",
    "    fdist[word] +=1\n",
    "    \n",
    "fdist\n",
    "#this will tell the number of  each word occured only once. To get total freuenccy of each word use :-  fdist[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Hi, There!': 1, 'We are in session.This session is based on NLP.': 1, 'hi, there!': 1, 'we are in session.this session is based on nlp.': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_token:\n",
    "    fdist[word.lower()] +=1\n",
    "    \n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3 TYPES OF TOKENS:-\n",
    " #BIGRAMS- TOKENS OF TWO CONSECUTIVE WRITTEN WORDS KNOWNS AS BIGRAM\n",
    " #Trigram:- Token of three consecutive written words known as Trigram\n",
    " #NGRAM:- Token of any number of consecutive words known as Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bigram' from 'nltk.util' (C:\\Users\\Lenovo\\Anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-398fc000c232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'bigram' from 'nltk.util' (C:\\Users\\Lenovo\\Anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\util.py)"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigram, trigram, ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI\n",
    "AI_tokens= nltk.word_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', ','),\n",
       " (',', 'There'),\n",
       " ('There', '!'),\n",
       " ('!', 'We'),\n",
       " ('We', 'are'),\n",
       " ('are', 'in'),\n",
       " ('in', 'session.This'),\n",
       " ('session.This', 'session'),\n",
       " ('session', 'is'),\n",
       " ('is', 'based'),\n",
       " ('based', 'on'),\n",
       " ('on', 'NLP'),\n",
       " ('NLP', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_bigram= list(nltk.bigrams(AI_tokens))\n",
    "AI_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', ',', 'There', '!', 'We', 'are', 'in'),\n",
       " (',', 'There', '!', 'We', 'are', 'in', 'session.This'),\n",
       " ('There', '!', 'We', 'are', 'in', 'session.This', 'session'),\n",
       " ('!', 'We', 'are', 'in', 'session.This', 'session', 'is'),\n",
       " ('We', 'are', 'in', 'session.This', 'session', 'is', 'based'),\n",
       " ('are', 'in', 'session.This', 'session', 'is', 'based', 'on'),\n",
       " ('in', 'session.This', 'session', 'is', 'based', 'on', 'NLP'),\n",
       " ('session.This', 'session', 'is', 'based', 'on', 'NLP', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_ngram=list(nltk.ngrams(AI_tokens,7)) # 7 here displays lists of word in row containing 7 tokens\n",
    "AI_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', ',', 'There'),\n",
       " (',', 'There', '!'),\n",
       " ('There', '!', 'We'),\n",
       " ('!', 'We', 'are'),\n",
       " ('We', 'are', 'in'),\n",
       " ('are', 'in', 'session.This'),\n",
       " ('in', 'session.This', 'session'),\n",
       " ('session.This', 'session', 'is'),\n",
       " ('session', 'is', 'based'),\n",
       " ('is', 'based', 'on'),\n",
       " ('based', 'on', 'NLP'),\n",
       " ('on', 'NLP', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_trigram= list(nltk.trigrams(AI_tokens))\n",
    "AI_trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMINGS:- changes to the token\n",
    " #normalize words into base form or root form\n",
    " #affection, affects, affections,affected, etc....\n",
    " #SteminGs algorith works by cutting ends or beginning of words tsking into account the most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem('having')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n",
      "eats:eat\n",
      "eating:eat\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','giving','given','gave','eats', 'eating']\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+pst.stem(words))\n",
    "#porter slice the word and make a meaningful word to understand from a list of words\n",
    "# fish:fish\n",
    "#fishes: fish\n",
    "#fisHing:fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n",
      "eats:eat\n",
      "eating:eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+lst.stem(words)) #Lancaster splice the word = fish: fi, fishing:fish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization :-IN case when steming don't results correctly. foreg;-FISH,fishing,fishes.\n",
    " it uses morphological analyses of wores. It groups together different inflected form of a word, called lemma. It somehow similar to stemming ,as it maps several word into common root. Output of lemmatization is a properword. foreg.:- a lemmaatizer should map ,\"go\", \"going\",\"gone\" into \"go\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_len=WordNetLemmatizer()\n",
    "word_len.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish:fish\n",
      "fishes:fish\n",
      "fisherman:fisherman\n",
      "fishing:fishing\n"
     ]
    }
   ],
   "source": [
    "words_to_lem=['fish','fishes','fisherman','fishing']\n",
    "for words in words_to_lem:\n",
    "    print(words+\":\"+word_len.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP WORDS:- \n",
    "several words in English like I,at,begin,for,got,know,various which are useful in making sentences but are not useful in NLP ,so these are called stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['αλλα',\n",
       " 'αν',\n",
       " 'αντι',\n",
       " 'απο',\n",
       " 'αυτα',\n",
       " 'αυτεσ',\n",
       " 'αυτη',\n",
       " 'αυτο',\n",
       " 'αυτοι',\n",
       " 'αυτοσ',\n",
       " 'αυτουσ',\n",
       " 'αυτων',\n",
       " 'αἱ',\n",
       " 'αἳ',\n",
       " 'αἵ',\n",
       " 'αὐτόσ',\n",
       " 'αὐτὸς',\n",
       " 'αὖ',\n",
       " 'γάρ',\n",
       " 'γα',\n",
       " 'γα^',\n",
       " 'γε',\n",
       " 'για',\n",
       " 'γοῦν',\n",
       " 'γὰρ',\n",
       " \"δ'\",\n",
       " 'δέ',\n",
       " 'δή',\n",
       " 'δαί',\n",
       " 'δαίσ',\n",
       " 'δαὶ',\n",
       " 'δαὶς',\n",
       " 'δε',\n",
       " 'δεν',\n",
       " \"δι'\",\n",
       " 'διά',\n",
       " 'διὰ',\n",
       " 'δὲ',\n",
       " 'δὴ',\n",
       " 'δ’',\n",
       " 'εαν',\n",
       " 'ειμαι',\n",
       " 'ειμαστε',\n",
       " 'ειναι',\n",
       " 'εισαι',\n",
       " 'ειστε',\n",
       " 'εκεινα',\n",
       " 'εκεινεσ',\n",
       " 'εκεινη',\n",
       " 'εκεινο',\n",
       " 'εκεινοι',\n",
       " 'εκεινοσ',\n",
       " 'εκεινουσ',\n",
       " 'εκεινων',\n",
       " 'ενω',\n",
       " 'επ',\n",
       " 'επι',\n",
       " 'εἰ',\n",
       " 'εἰμί',\n",
       " 'εἰμὶ',\n",
       " 'εἰς',\n",
       " 'εἰσ',\n",
       " 'εἴ',\n",
       " 'εἴμι',\n",
       " 'εἴτε',\n",
       " 'η',\n",
       " 'θα',\n",
       " 'ισωσ',\n",
       " 'κ',\n",
       " 'καί',\n",
       " 'καίτοι',\n",
       " 'καθ',\n",
       " 'και',\n",
       " 'κατ',\n",
       " 'κατά',\n",
       " 'κατα',\n",
       " 'κατὰ',\n",
       " 'καὶ',\n",
       " 'κι',\n",
       " 'κἀν',\n",
       " 'κἂν',\n",
       " 'μέν',\n",
       " 'μή',\n",
       " 'μήτε',\n",
       " 'μα',\n",
       " 'με',\n",
       " 'μεθ',\n",
       " 'μετ',\n",
       " 'μετά',\n",
       " 'μετα',\n",
       " 'μετὰ',\n",
       " 'μη',\n",
       " 'μην',\n",
       " 'μἐν',\n",
       " 'μὲν',\n",
       " 'μὴ',\n",
       " 'μὴν',\n",
       " 'να',\n",
       " 'ο',\n",
       " 'οι',\n",
       " 'ομωσ',\n",
       " 'οπωσ',\n",
       " 'οσο',\n",
       " 'οτι',\n",
       " 'οἱ',\n",
       " 'οἳ',\n",
       " 'οἷς',\n",
       " 'οὐ',\n",
       " 'οὐδ',\n",
       " 'οὐδέ',\n",
       " 'οὐδείσ',\n",
       " 'οὐδεὶς',\n",
       " 'οὐδὲ',\n",
       " 'οὐδὲν',\n",
       " 'οὐκ',\n",
       " 'οὐχ',\n",
       " 'οὐχὶ',\n",
       " 'οὓς',\n",
       " 'οὔτε',\n",
       " 'οὕτω',\n",
       " 'οὕτως',\n",
       " 'οὕτωσ',\n",
       " 'οὖν',\n",
       " 'οὗ',\n",
       " 'οὗτος',\n",
       " 'οὗτοσ',\n",
       " 'παρ',\n",
       " 'παρά',\n",
       " 'παρα',\n",
       " 'παρὰ',\n",
       " 'περί',\n",
       " 'περὶ',\n",
       " 'ποια',\n",
       " 'ποιεσ',\n",
       " 'ποιο',\n",
       " 'ποιοι',\n",
       " 'ποιοσ',\n",
       " 'ποιουσ',\n",
       " 'ποιων',\n",
       " 'ποτε',\n",
       " 'που',\n",
       " 'ποῦ',\n",
       " 'προ',\n",
       " 'προσ',\n",
       " 'πρόσ',\n",
       " 'πρὸ',\n",
       " 'πρὸς',\n",
       " 'πως',\n",
       " 'πωσ',\n",
       " 'σε',\n",
       " 'στη',\n",
       " 'στην',\n",
       " 'στο',\n",
       " 'στον',\n",
       " 'σόσ',\n",
       " 'σύ',\n",
       " 'σύν',\n",
       " 'σὸς',\n",
       " 'σὺ',\n",
       " 'σὺν',\n",
       " 'τά',\n",
       " 'τήν',\n",
       " 'τί',\n",
       " 'τίς',\n",
       " 'τίσ',\n",
       " 'τα',\n",
       " 'ταῖς',\n",
       " 'τε',\n",
       " 'την',\n",
       " 'τησ',\n",
       " 'τι',\n",
       " 'τινα',\n",
       " 'τις',\n",
       " 'τισ',\n",
       " 'το',\n",
       " 'τοί',\n",
       " 'τοι',\n",
       " 'τοιοῦτος',\n",
       " 'τοιοῦτοσ',\n",
       " 'τον',\n",
       " 'τοτε',\n",
       " 'του',\n",
       " 'τούσ',\n",
       " 'τοὺς',\n",
       " 'τοῖς',\n",
       " 'τοῦ',\n",
       " 'των',\n",
       " 'τό',\n",
       " 'τόν',\n",
       " 'τότε',\n",
       " 'τὰ',\n",
       " 'τὰς',\n",
       " 'τὴν',\n",
       " 'τὸ',\n",
       " 'τὸν',\n",
       " 'τῆς',\n",
       " 'τῆσ',\n",
       " 'τῇ',\n",
       " 'τῶν',\n",
       " 'τῷ',\n",
       " 'ωσ',\n",
       " \"ἀλλ'\",\n",
       " 'ἀλλά',\n",
       " 'ἀλλὰ',\n",
       " 'ἀλλ’',\n",
       " 'ἀπ',\n",
       " 'ἀπό',\n",
       " 'ἀπὸ',\n",
       " 'ἀφ',\n",
       " 'ἂν',\n",
       " 'ἃ',\n",
       " 'ἄλλος',\n",
       " 'ἄλλοσ',\n",
       " 'ἄν',\n",
       " 'ἄρα',\n",
       " 'ἅμα',\n",
       " 'ἐάν',\n",
       " 'ἐγώ',\n",
       " 'ἐγὼ',\n",
       " 'ἐκ',\n",
       " 'ἐμόσ',\n",
       " 'ἐμὸς',\n",
       " 'ἐν',\n",
       " 'ἐξ',\n",
       " 'ἐπί',\n",
       " 'ἐπεὶ',\n",
       " 'ἐπὶ',\n",
       " 'ἐστι',\n",
       " 'ἐφ',\n",
       " 'ἐὰν',\n",
       " 'ἑαυτοῦ',\n",
       " 'ἔτι',\n",
       " 'ἡ',\n",
       " 'ἢ',\n",
       " 'ἣ',\n",
       " 'ἤ',\n",
       " 'ἥ',\n",
       " 'ἧς',\n",
       " 'ἵνα',\n",
       " 'ὁ',\n",
       " 'ὃ',\n",
       " 'ὃν',\n",
       " 'ὃς',\n",
       " 'ὅ',\n",
       " 'ὅδε',\n",
       " 'ὅθεν',\n",
       " 'ὅπερ',\n",
       " 'ὅς',\n",
       " 'ὅσ',\n",
       " 'ὅστις',\n",
       " 'ὅστισ',\n",
       " 'ὅτε',\n",
       " 'ὅτι',\n",
       " 'ὑμόσ',\n",
       " 'ὑπ',\n",
       " 'ὑπέρ',\n",
       " 'ὑπό',\n",
       " 'ὑπὲρ',\n",
       " 'ὑπὸ',\n",
       " 'ὡς',\n",
       " 'ὡσ',\n",
       " 'ὥς',\n",
       " 'ὥστε',\n",
       " 'ὦ',\n",
       " 'ᾧ']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi  There!', 'We are in session This session is based on NLP ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "punctuation=re.compile(r'[-,.?;:()/\\|0-9]')\n",
    "post=[]\n",
    "for words in AI_token:\n",
    "    word=punctuation.sub(' ', words)\n",
    "    if len(word)>0:\n",
    "        post.append(word)\n",
    "print(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS:- PARTS OF SPEECH\n",
    "used to describe whether a word is a noun,an adjective, a proper noin, singular,plural ,verb,adverb, symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'This', 'is', 'UjjwAl', 'in', 'auditorium']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent= 'Hello, This is UjjwAl in auditorium'\n",
    "sent_tokens=word_tokenize(sent)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN')]\n",
      "[(',', ',')]\n",
      "[('This', 'DT')]\n",
      "[('is', 'VBZ')]\n",
      "[('UjjwAl', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('auditorium', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ujjwal', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('walking', 'VBG')]\n",
      "[('in', 'IN')]\n",
      "[('park', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent2='Ujjwal is walking in park'\n",
    "sent2_tokens=word_tokenize(sent2)\n",
    "for token in sent2_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition:-\n",
    "naming such as movie,monetary value, organization,location,quantities,person from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "NE_sent=\" the INdian ppoliticians shouts in parliament \"\n",
    "NE_tokens=word_tokenize(NE_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tags=nltk.pos_tag(NE_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  the/DT\n",
      "  (GPE INdian/JJ)\n",
      "  ppoliticians/NNS\n",
      "  shouts/NNS\n",
      "  in/IN\n",
      "  parliament/NN)\n"
     ]
    }
   ],
   "source": [
    "NE_NER=ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NER ENTITIES LIST:-  facility, location,Organization, geo-socio political group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# syntax  tree is a tree representation of sytactic structure of sentences /strings.\n",
    "# chunking:-picking up individual :- used where words are used asadjective,noun,verb etc..  together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=\"the tall tower can be seen in big city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens=nltk.pos_tag(word_tokenize(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_np=\"NP: {<DT><jj><NN>}\"\n",
    "chunk_parser=nltk.RegexpParser(grammer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 1 stages>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP: {<DT><jj><NN>}'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammer_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  the/DT\n",
      "  tall/JJ\n",
      "  tower/NN\n",
      "  can/MD\n",
      "  be/VB\n",
      "  seen/VBN\n",
      "  in/IN\n",
      "  big/JJ\n",
      "  city/NN)\n"
     ]
    }
   ],
   "source": [
    "chunk_result=chunk_parser.parse(new_tokens)\n",
    "print(chunk_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE DETECTION WITH EYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-c42dcbe253e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwebcam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mwebcam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfaceCascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cascpath='S:/machine learning/build-face-dataset/haarcascade_frontalface_default.xml'\n",
    "faceCascade=cv2.CascadeClassifier(cascpath)\n",
    "webcam=cv2.VideoCapture(0)\n",
    "(_, im)= webcam.read()\n",
    "gray=cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "faces=faceCascade.detectMultiScale(gray, scaleFactor=1.1,minNeighbors=5, minSize=(30,30),flags= cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(im, (x,y), (x+w,y+h),(0,255,0), 2)\n",
    "\n",
    "cv2.imshow('Faces Found', im)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
